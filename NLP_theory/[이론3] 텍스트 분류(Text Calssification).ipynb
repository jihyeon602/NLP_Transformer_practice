{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [이론3] 텍스트 분류(Text Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 목표\n",
    "- 텍스트 문서 표현에 대해 살펴봅시다.\n",
    "- 텍스트 분류기에 대해 살펴봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "\n",
    "### [이론3] 텍스트 분류(Text Classification)\n",
    "1. 소개\n",
    "2. 문서 표현\n",
    "3. 텍스트 분류기\n",
    "4. 텍스트 분류기 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  [이론3] 텍스트 분류(Text Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지난 시간에 사람과 컴퓨터의 커뮤니케이션에 있어 자연어처리가 사용됨을 배웠습니다. 그 형태는 아래 그림과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NLP.jpg\" width=\"50%\" height=\"50%\" title=\"NLP\" alt=\"NLP\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 시간에는 이 중 사람이 생성한 자연어(Natural Language)를 분석(Analysis)하고 이해하는 문제 중 하나인 텍스트 분류(Text Classification)에 대해 배우겠습니다.\n",
    "\n",
    "텍스트 분류는 입력으로 주어진 텍스트를 받아 이를 이해한 후 정해진 클래스로 분류하는 것을 의미합니다. 대체로 텍스트의 크기는 여러 문장, 문단이 합쳐진 문서(document)라고 가정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--<img src=\"img/SA_1.jpg\" width=\"50%\" height=\"50%\" title=\"SA_1\" alt=\"SA_1\"></img>-->\n",
    "<img src=\"img/SA_2.jpg\" width=\"50%\" height=\"50%\" title=\"SA_2\" alt=\"SA_2\"></img>\n",
    "\n",
    "출처: https://movie.naver.com/movie/bi/mi/basic.nhn?code=17421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 분류의 유명한 예시 중 하나로는, 감정 분석(Sentiment Analysis)이 있습니다.\n",
    "- 영화 리뷰 텍스트를 보고 그 리뷰가 영화에 대해 긍정적인지 부정적인지를 판단하는 것을 예시로 들 수 있습니다.\n",
    "- 영화 리뷰 텍스트 분류는 분류기(Classifier)의 입력으로 영화 리뷰 텍스트를 입력받고, 긍정과 부정 중 하나를 출력으로 내놓습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 문서 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 분류의 입력은 텍스트가 적혀진 문서입니다. 그럼 먼저 이 문서를 컴퓨터가 이해하는 표현(Representation)으로 바꾸는 것이 필요합니다.\n",
    "\n",
    "지난 시간 우리는 단어의 표현에 대해 배웠습니다.\n",
    "\n",
    "이는 텍스트의 기본 단위를 단어라고 볼 수 있기 때문이며 이러한 단어 표현을 이용하여 문서 표현에 대해 살펴보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Network\n",
    "\n",
    "문서를 표현하는 방법은 다양합니다.\n",
    "\n",
    "이번 시간에는 그 중 하나인 Recurrent Neural Network(RNN)을 이용하여 문서를 표현하고 분류하는 방법을 배우겠습니다.\n",
    "\n",
    "RNN은 연속적으로 나타나는 데이터를 모델링하는데 주로 사용됩니다. 텍스트는 단어가 연속적으로 나타나는 것으로 볼 수 있기 때문에, 텍스트를 모델링 하는데도 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/RNN_1.png\" width=\"75%\" height=\"75%\" title=\"RNN_1\" alt=\"RNN_1\"></img>\n",
    "\n",
    "출처: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림은 RNN의 구조를 나타낸 것입니다.\n",
    "\n",
    "RNN은 루프(loop)를 가지고 있으며, 각 시간 단계(time step)에서 이전 시간 단계의 숨겨진 상태 $a^{<t-1>}$와 현재 입력 $x^{<t>}$를 조합하여 현재 시간 단계의 숨겨진 상태 $a^{<t>}$를 계산합니다.\n",
    "\n",
    "파란색 부분은 숨겨진 상태 $a^{<t>}$를 나타내며, 이는 시퀀스의 정보를 압축하고 이전 정보를 현재 시간 단계로 전달하는 역할을 합니다.\n",
    "\n",
    "- $a^{<0>}$는 초기 숨겨진 상태로, RNN의 첫 번째 시간 단계에서 사용됩니다.\n",
    "- $x^{<t>}$는 입력 시퀀스의 t번째 요소를 나타내며, 각 시간 단계에서 RNN에 새로운 정보를 주입하는 역할을 합니다.\n",
    "- $y^{<t>}$는 RNN의 출력으로, 숨겨진 상태 $a^{<t>}$를 기반으로 계산됩니다.\n",
    "\n",
    "RNN은 시퀀스의 길이에 따라 반복적으로 숨겨진 상태와 출력을 생성합니다.\n",
    "\n",
    "이러한 구조를 통해 RNN은 순차적인 데이터, 예를 들어 문장이나 시계열 데이터와 같은 시퀀스 데이터를 처리하는 데 효과적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN을 텍스트 분류에 사용할 때는, 단어 혹은 토큰 단위로 입력합니다. 즉, $x^{<t>}$은 $t$ 번째 단어 혹은 토큰을 뜻합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/RNN_2.png\" width=\"50%\" height=\"50%\" title=\"RNN_2\" alt=\"RNN_2\"></img>\n",
    "\n",
    "출처: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 RNN이 내놓는 출력들이 다양하지만 문서 표현을 구하고자 할 때는 대체로 마지막 단어까지 입력으로 주어 나온 출력을 이용합니다.\n",
    "\n",
    "위의 그림에서처럼 입력을 계속해서 주면 RNN은 그 입력값과 그들의 순서까지 함께 고려하여 최종적으로 입력 데이터의 출력값인 $\\hat{y}$를 내놓습니다. 이 출력값을 입력한 문서를 표현하는 벡터로 간주할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 50)          50000     \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 66,110\n",
      "Trainable params: 66,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Embedding(input_dim=1000, output_dim=50))\n",
    "\n",
    "model.add(tf.keras.layers.SimpleRNN(100))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드는 TensorFlow를 사용하여 RNN 모델을 구성한 예시입니다.\n",
    "\n",
    "먼저, 시계열 데이터를 모델링하기 위해 Sequential 모델을 사용합니다.\n",
    "\n",
    "그 다음, Embedding 레이어를 생성합니다. Embedding의 입력 크기는 1000이며 출력 크기는 50입니다.\n",
    "\n",
    "이 Embedding 레이어는 단어를 벡터 형태로 변환하는 역할을 합니다. 입력으로는 1000 차원의 벡터로 표현된 단어가 들어오며, 각 단어는 50차원의 벡터로 변환됩니다.\n",
    "실제 데이터를 사용할 때는 데이터 내의 단어 개수를 파악하여 이를 입력으로 설정할 수 있습니다.\n",
    "\n",
    "그 다음으로는 SimpleRNN 레이어를 사용하여 RNN을 구성합니다. 해당 RNN의 hidden size는 100입니다.\n",
    "\n",
    "그리고 마지막으로, fully connected 네트워크를 거쳐 최종 출력으로 크기가 10인 벡터를 얻습니다.\n",
    "\n",
    "요약하자면, 이 모델은 단어 시계열 데이터를 입력으로 받아 RNN을 거쳐 최종적으로 10차원의 벡터를 출력하는 구조로 이루어져 있습니다. 이를 통해 단어 시퀀스를 벡터로 잘 표현하는 모델을 구성한 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long-Short Term Memory\n",
    "\n",
    "RNN에는 앞서 소개한 간단한 형태부터 좀 더 복잡한 것까지 다양합니다.\n",
    "\n",
    "그 중 우리는 Long-Short Term Memory(LSTM)에 대해 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 RNN의 경우 문제점이 하나 있습니다.\n",
    "\n",
    "바로 입력들 간의 거리가 멀면 그들 사이의 관계를 잊어버린다는 것입니다.\n",
    "\n",
    "<img src=\"img/RNN_1.png\" width=\"75%\" height=\"75%\" title=\"RNN_1\" alt=\"RNN_1\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어 위의 그림에서 $x^{<1>}$과 $x^{<t>}$는 그 사이에 많은 입력값이 있습니다.\n",
    "\n",
    "이에 $x^{<t>}$의 출력인 $y^{<t>}$를 만들 때 $x^{<1>}$의 정보는 거의 영향력이 없습니다. 이것은 어쩌면 당연할 수도 있습니다, 저희는 어떤 단어에 대해 알기 위해서는 그 근처의 단어들만을 이용하여도 된다고 배웠습니다.\n",
    "\n",
    "하지만, 아래와 같은 예시 글을 생각해봅시다.\n",
    "\n",
    "`학생은 말했다. \"미래에 중요한 가치는 인공지능에서 나올 것이다. 이처럼 인공지능에 대한 공부는 지금 매우 중요하다. 따라서 나는 인공지능을 공부한다.\"`\n",
    "\n",
    "위의 글에서 `학생`과 `나`는 동일한 대상입니다. 하지만 `나`라는 단어와 `학생`이라는 단어 사이에는 많은 단어들이 있습니다. 그렇기에 단순히 `나`에 대해 이해하기 위해 근처에 있는 단어들을 살펴본다면 누구인지를 알 수 없습니다. 대신 앞에 나왔던 `학생`이라는 단어에 대해 따로 기억한 후 `나`라는 단어를 만났을 때 이를 꺼내야 하는 것입니다.\n",
    "\n",
    "RNN은 각 시점마다 하나의 은닉 상태만을 갖고 있기 때문에 긴 시퀀스에서 먼 위치에 있는 정보를 적절하게 기억하는 데 어려움이 있습니다. 이로 인해 `나`와 `학생` 사이에 있는 많은 단어들로 인해 서로 간의 영향력이 없어지는 문제가 발생합니다. 이를 **장기 의존성 문제** 라고 합니다.\n",
    "\n",
    "LSTM은 이러한 문제를 극복하기 위해 고안된 RNN의 변형 모델입니다. LSTM은 각 시점마다 은닉 상태뿐만 아니라, 입력 정보 중에서 긴 시간 간격을 가진 정보도 기억하기 위한 추가적인 메모리 셀을 갖습니다. 이 메모리 셀은 입력 정보 중에서 중요한 정보를 장기적으로 기억하고, 필요할 때마다 해당 정보를 활용하여 출력을 조절하는 역할을 합니다. 메모리 셀은 이전 시점의 정보를 오래 기억해두고 새로운 입력을 받아 적절하게 현재 상태를 업데이트함으로써, 더 긴 시퀀스에서도 의미 있는 정보를 유지할 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 50)          50000     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 111,410\n",
      "Trainable params: 111,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Embedding(input_dim=1000, output_dim=50))\n",
    "\n",
    "model.add(tf.keras.layers.LSTM(100))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드느 앞서 살펴본 RNN을 LSTM으로 바꾼 것입니다.\n",
    "\n",
    "전반적으로 동일하며 모델이 RNN에서 LSTM만이 바뀌었다는 것을 알 수 있습니다.\n",
    "\n",
    "더하여 summary에 나오는 파라메터 크기를 보면 RNN일 때는 15100인 것이 LSTM일 때는 60400으로 커진 것을 알 수 있습니다.\n",
    "\n",
    "그만큼 조금 더 복잡한 모델이지만 long term memory와 short term memory로 구성되어 전체적인 내용을 잘 이해하도록 한 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 RNN/LSTM을 이용하여 텍스트 입력을 받아 텍스트 문서의 표현을 만들 수 있음을 확인했습니다. 여기서는 간단히 RNN 혹은 LSTM을 가지고 분류기에 적합한 문서 표현을 설명하였습니다.\n",
    "\n",
    "하지만 어떤 RNN을 사용하는지에 따라 어떻게 RNN을 구성할 것인지에 따라 그리고 풀고자 하는 자연어처리 문제에 따라 문서 표현을 만드는 방법은 매우 다양합니다. 실습과 함께 좀 더 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 텍스트 분류기(Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 살펴본 문서 표현으로 컴퓨터가 적절한 클래스를 찾아 분류하는 텍스트 분류기에 대해 알아보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully Connected Network\n",
    "\n",
    "신경망(Neural network)을 이용하여 분류기를 구현할 때, 사용할 수 가장 있는 간단한 아키텍처는 Fully Connected Network입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/FCN.png\" width=\"50%\" height=\"50%\" title=\"FCN\" alt=\"FCN\"></img>\n",
    "\n",
    "출처: https://www.oreilly.com/library/view/tensorflow-for-deep/9781491980446/ch04.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림은 Fully Connected Network를 나타낸 것입니다.\n",
    "\n",
    "왼쪽의 노드들과 오른쪽에 노드들은 전부 연결되어 있습니다. 그래서 fully conncected라고 불립니다.\n",
    "\n",
    "왼쪽의 노드(입력 노드)들로부터 입력을 받으면 그 값들이 연결망을 통해 오른쪽 노드(출력 노드)들로 전달되고, 오른쪽의 출력 노드에서는 결과값을 내놓습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류기의 출력은 어떤 클래스인지를 맞추는 것입니다.\n",
    "\n",
    "따라서 Fully Connected Network의 오른쪽인 출력 레이어의 노드의 수는 문제에 정의된 클래스의 개수와 같습니다.\n",
    "\n",
    "앞의 예에서 보면 분류기는 리뷰글을 입력으로 받아 출력으로 긍정인지 부정인지 두 개의 클래스를 내놓아야 합니다. 그렇기에 출력 노드에 두 개의 노드가 각각 긍정 혹은 부정을 담당하게 되는 것입니다.우리는 노드에서 나오는 값 중 가장 높은것을 판단함으로써 분류기의 최종 결과(클래스)를 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어 아래와 같은 리뷰 데이터가 있다고 할 때 분류기는 아래와 같은 값을 내놓습니다.\n",
    "\n",
    "`내 생애 최고의 영화` => 긍정 노드: 140, 부정 노드: 60\n",
    "\n",
    "`보면서 잠만 잤어요` => 긍정 노드: 40000, 부정 노드: 160000\n",
    "\n",
    "그럼 우리는 분류기가 첫 번째 데이터에 대해서는 긍정이라고, 두 번째 데이터에 대해서는 부정이라고 분류함을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Text_Classifier.jpg\" width=\"75%\" height=\"75%\" title=\"Text_Classifier\" alt=\"Text_Classifier\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN을 이용한 텍스트 분류기의 전체적인 구조는 위의 그림과 같습니다.\n",
    "\n",
    "먼저 텍스트 문서를 입력으로 받아 이를 RNN을 이용하여 문서 표현을 만듭니다.\n",
    "\n",
    "그리고 Fully Connected Network(FCN)을 거치게 되면 우리가 원하는 클래스 값을 얻게 되는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 텍스트 분류기 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 파트에서는 텍스트 분류기 학습에 대해 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류기 학습에는 레이블(label)이 달린 데이터가 필요합니다.\n",
    "\n",
    "여기서 레이블이란 (대체로) 사람이 달아놓은 데이터에 대한 클래스 값을 뜻합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어 아래와 같은 긍정과 부정 클래스를 가진 리뷰 데이터가 있습니다.\n",
    "\n",
    "`살면서 꼭 봐야할 영화 (긍정)`\n",
    "\n",
    "`다시는 보고 싶지 않는 영화 (부정)`\n",
    "\n",
    "위의 두 글에서 우리는 첫 번째는 긍정적인 감정 클래스를 두 번째는 부정적인 감정 클래스를 가짐을 알 수 있습니다.\n",
    "\n",
    "그럼 이런 클래스 정보를 데이터마다 붙이게 되면 그것이 레이블이 되는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이처럼 분류기의 학습에는 레이블이 달린 데이터를 필요로 합니다.\n",
    "\n",
    "왜냐하면 학습된 분류기는 레이블이 없는 데이터를 읽어 그 데이터의 클래스를 맞추는 작업을 하기 때문입니다.\n",
    "\n",
    "`살면서 총 16번 봤다.`\n",
    "\n",
    "위와 같은 레이블이 없는 리뷰가 들어왔을 때 분류기가 긍정 감정 클래스를 추론하기를 바랍니다.\n",
    "\n",
    "이렇게 분류기를 동작시키기 위해서는 레이블 데이터를 학습시켜야합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류기 학습의 아이디어는 다음과 같습니다.\n",
    "\n",
    "> 분류기가 추론한 결과와 실제 결과(레이블)의 차이를 줄이자\n",
    "\n",
    "어떻게 보면 당연한 얘기입니다.\n",
    "\n",
    "분류기가 제대로 학습되어 동작한다는 것은 실제 결과인 레이블을 잘 맞춘다는 것이니까요.\n",
    "\n",
    "그렇기에 잘 맞추도록 즉, 분류기가 추론한 결과와 실제 결과의 차이를 줄이도록 하는 것이 바로 분류기를 학습시킨다고 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과의 차이를 줄이기에 앞서 출력 노드가 내놓는 값들을 정리하여 확률 분포를 가지도록 하겠습니다.\n",
    "\n",
    "예를 들어 분류기는 아래와 같은 값을 내놓습니다.\n",
    "\n",
    "`살면서 꼭 봐야할 영화` => 긍정 노드: 96, 부정 노드: 224\n",
    "\n",
    "`다시는 보고 싶지 않는 영화` => 긍정 노드:6216 , 부정 노드: 1554\n",
    "\n",
    "이렇게 노드가 내놓는 값이 너무 차이가 나면 실제 데이터와 비교하기 곤란합니다.\n",
    "\n",
    "따라서 이를 정리하여 합쳐서 1의 값을 가지도록 하는 확률 분포로 바꿉니다.\n",
    "\n",
    "`살면서 꼭 봐야할 영화` => 긍정 노드: 0.3, 부정 노드: 0.7\n",
    "\n",
    "`다시는 보고 싶지 않는 영화` => 긍정 노드: 0.8, 부정 노드: 0.2\n",
    "\n",
    "우리는 여전히 높은 값을 가지는 노드를 찾으면 되는 것이기에 최종 분류 결과에는 변화가 없습니다.\n",
    "\n",
    "하지만 분류기가 잘못된 분류를 하는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 실제 결과(레이블)도 확률 분포를 가지도록 하겠습니다.\n",
    "\n",
    "`살면서 꼭 봐야할 영화` => 긍정: 1.0, 부정: 0.0\n",
    "\n",
    "`다시는 보고 싶지 않는 영화` => 긍정: 0.0, 부정: 1.0\n",
    "\n",
    "이렇게 설정한 이유는 첫 번째 데이터는 긍정이고, 두 번째 데이터는 부정이라는 것을 레이블을 통해 알 수 있기 때문입니다.\n",
    "\n",
    "따라서 그 값만을 높게 설정한 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이 두 결과들의 차이를 줄여야 합니다.\n",
    "\n",
    "그렇다면 확률 분포의 차이는 무엇일까요?\n",
    "\n",
    "- 0.3과 0.7 확률 분포와 1.0과 0.0 확률 분포의 차이\n",
    "\n",
    "- 0.8과 0.2 확률 분포와 0.0과 1.0 확률 분포의 차이\n",
    "\n",
    "이러한 두 확률 분포의 차이를 나타내는 것이 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수학적으로 여러 방법으로 두 확률 분포의 차이를 표기합니다.\n",
    "\n",
    "하지만 대체적으로 많이 쓰이는 것은 Cross entropy라 불리우는 방법으로 두 확률 분포의 차이를 계산합니다.\n",
    "\n",
    "그리고 계산된 확률 분포의 차이를 최소화하도록 에러 역전파(Error Backpropagation) 기법을 통해 neural network 전체를 학습시킵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 자료에서는 전반적인 내용에 대해 살펴보았습니다.\n",
    "\n",
    "좀 더 자세한 내용과 실제 데이터 사용은 실습에서 살펴보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:rgb(120, 120, 120)\">본 학습 자료를 포함한 사이트 내 모든 자료의 저작권은 엘리스에 있으며 외부로의 무단 복제, 배포 및 전송을 불허합니다.\n",
    "\n",
    "Copyright @ elice all rights reserved</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
