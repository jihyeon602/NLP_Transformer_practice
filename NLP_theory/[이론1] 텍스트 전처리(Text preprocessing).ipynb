{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [이론1] 텍스트 전처리(Text preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 목표\n",
    "- 자연어 데이터인 텍스트 전처리의 필요성을 살펴봅시다.\n",
    "- 어떠한 텍스트 전처리 기술이 있는지 살펴봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "\n",
    "### [이론1] 텍스트 전처리(Text preprocessing)\n",
    "1. 자연어처리 소개\n",
    "2. 텍스트 전처리 이유\n",
    "3. 텍스트 전처리 기술"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  [이론1] 텍스트 전처리(Text preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 자연어처리 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어처리(Natural Language Processing, NLP)란 사람과 컴퓨터가 커뮤니케이션을 할 때 마치 사람과 사람끼리 커뮤니케이션을 하는 것처럼 자연어로 커뮤니케이션을 하도록 하는 일련의 작업을 뜻합니다. \n",
    "\n",
    "이 일련의 작업은 크게 아래 그림과 같이 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/NLP.jpg\" width=\"50%\" height=\"50%\" title=\"NLP\" alt=\"NLP\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림은 사람과 컴퓨터가 자연어로 커뮤니케이션을 할 때의 모습입니다.\n",
    "\n",
    "사람은 자연어(Natural Language)를 통해 본인의 의사나 정보를 표현합니다.\n",
    "\n",
    "컴퓨터는 이를 분석(Analysis)을 통해 자연어를 컴퓨터만의 표현(Representation)으로 바꾸어 이해합니다.\n",
    "\n",
    "일련의 작업 후 컴퓨터는 사람에게 결과를 전달하고자 그 표현에서 자연어를 생성(Generation)을 하여 사람과 커뮤니케이션을 하는 것입니다.\n",
    "\n",
    "각각의 과정은 아래와 같이 정리할 수 있습니다.\n",
    "\n",
    "- Analysis\n",
    "  - 컴퓨터가 자연어를 이해하는 과정\n",
    "  - 자연어를 컴퓨터의 표현(representation)으로 바꾸어 자연어를 이해함\n",
    "  - 사용 예: 텍스트 분류\n",
    "- Generation\n",
    "  - 컴퓨터가 자연어를 생성하는 과정\n",
    "  - 컴퓨터의 표현에서 자연어로 바꾸어 사람이 이해하도록 함\n",
    "  - 사용 예: 요약문 작성\n",
    "- Representation\n",
    "  - 컴퓨터가 자연어를 표현하는 과정\n",
    "  - 사람이 정보를 자연어로 표현하듯 컴퓨터만의 표현을 가짐\n",
    "  - 사용 예: 텍스트 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 자연어처리 목적\n",
    "\n",
    "이처럼 우리는 자연어처리를 통해 컴퓨터와 자연어로 커뮤니케이션을 할 수 있습니다.\n",
    "\n",
    "더하여 자연어처리를 통해 컴퓨터를 이 세상의 지식, 경험, 정보를 학습시킬 수 있습니다.\n",
    "\n",
    "생각해봅시다. \n",
    "\n",
    "이 세상의 지식, 경험, 정보를 학습할 때 우리는 책을 읽습니다. \n",
    "\n",
    "책에는 글이 적혀있고 그 글에는 그러한 지식, 경험, 정보가 담겨져있습니다. \n",
    "\n",
    "그 글을 읽음으로써 우리는 이들을 학습합니다. 지금 여러분들이 이 글을 읽고 학습하는 것처럼 말입니다.\n",
    "\n",
    "같은 방법으로 우리는 글을 통해 컴퓨터를 학습시킬 수 있습니다. \n",
    "\n",
    "그렇다면 당연히 컴퓨터가 글을 읽고 이해하도록 해야할 것이며 이 때 자연어처리가 반드시 필요하게 되는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 자연어처리 어플리케이션\n",
    "\n",
    "자연어처리 분야의 어플리케이션은 다양합니다. 간단하게 열거하자면 아래와 같습니다.\n",
    "\n",
    "- Machine Translation\n",
    "- Information Retrieval\n",
    "- Question Answering\n",
    "- Dialogue Systems\n",
    "- Information Extraction\n",
    "- Summarization\n",
    "- Sentiment Analysis\n",
    "- ...\n",
    "\n",
    "아쉽지만 우리는 이 모든 것을 여기서 다루지는 못합니다. 하지만 이들을 하기 위한 기초를 이번 수업에서 배우게 될 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 자연어처리 데이터\n",
    "\n",
    "자연어처리 분야 데이터 역시 다양합니다. 텍스트 데이터라면 그 어떤 것이든 데이터가 될 수 있다고 볼 수 있습니다.\n",
    "\n",
    "- News articles\n",
    "- Blog articles\n",
    "- Wikipedia articles\n",
    "- Online social media (e.g. Twitter) messages\n",
    "- Books\n",
    "- Documents\n",
    "- Papers\n",
    "- Conversations\n",
    "- ...\n",
    "\n",
    "이처럼 다양한 자연어처리 데이터가 있습니다. \n",
    "\n",
    "이들을 위의 어플리케이션에 맞춰 작업을 할 때 공통적으로 먼저 우리는 전처리(preprocessing) 작업을 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 텍스트 전처리 목적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 파트에서는 자연어처리 분야에 있어 공통의 시작 단계인 텍스트 전처리에 대해 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 전처리란 간단히 말하자면 실제 텍스트 데이터를 컴퓨터가 이해하기 쉽도록 바꾸는 작업을 뜻합니다.\n",
    "\n",
    "대표적으로 데이터 내 노이즈(noise)를 제거하는 것이 있을 수 있습니다.\n",
    "\n",
    "예를 들어 인터넷 뉴스 기사 데이터를 생각해봅시다.\n",
    "\n",
    "아래 그림은 어떤 인터넷 뉴스 기사의 내용입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/news1.jpg\" width=\"50%\" height=\"50%\" title=\"news1\" alt=\"news1\"></img>\n",
    "\n",
    "출처: https://news.v.daum.net/v/20201025080011013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 기사는 우리가 읽기 쉽게 웹브라우저에서 보이는 형태입니다.\n",
    "\n",
    "하지만 실제 우리가 데이터 수집을 통해 html 파일을 다운로드 받으면 이는 아래와 같이 되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/news2.jpg\" width=\"50%\" height=\"50%\" title=\"news2\" alt=\"news1\"></img>\n",
    "\n",
    "출처: https://news.v.daum.net/v/20201025080011013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 기사를 이해하기 위해서는 웹브라우저에 보이는대로 본문만이 필요합니다.\n",
    "\n",
    "하지만 실제 데이터는 html로 작성되어있기에 html 태그들이 함께 있습니다.\n",
    "\n",
    "이런 노이즈들은 텍스트를 읽고 이해함에 있어 불필요합니다. 이들을 제거하지 않고 자연어처리 작업을 수행할 경우 그 성능이 좋지 못하게 되죠.\n",
    "\n",
    "따라서 우리는 이러한 노이즈를 제거하고 컴퓨터가 텍스트를 좀 더 이해하기 수월하게 하는 전처리 과정을 자연어처리 분야 공통으로 수행하여야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 텍스트 전처리 기술"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 파트에서는 텍스트 전처리의 기술들에 대해 자세히 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 전처리 기술은 다양합니다. 하지만 대표적으로 많이들 쓰이는 기술들은 아래와 같습니다.\n",
    "\n",
    "- 소문자로 통일(영어)\n",
    "- 노이즈 제거\n",
    "- 토큰화(Tokenization)\n",
    "- stop-words 제거\n",
    "- 표준화(Normalization)\n",
    "\n",
    "아래에 하나씩 살펴보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 소문자로 통일\n",
    "처음으로 살펴볼 전처리 기술은 소문자 통일입니다.\n",
    "\n",
    "이는 라틴 알파벳으로 적혀진 텍스트(대표적으로 영어)에 주로 사용하는 방법입니다.\n",
    "\n",
    "이를 행하는 이유는 아래와 같은 예시를 살펴봅시다\n",
    "\n",
    "- korea\n",
    "- Korea\n",
    "- KOREA\n",
    "\n",
    "위 세 단어들의 경우 우리는 모두 같은 뜻의 단어인 Korea임을 알 수 있습니다. \n",
    "\n",
    "하지만 이를 그대로 사용할 경우 컴퓨터는 자칫 세 개의 서로 다른 단어로 착각할 수 있습니다. 그런 경우 아무래도 성능 저하를 일으키게 됩니다.\n",
    "\n",
    "이를 해결하는 방법은 간단히 모든 문자를 소문자로 통일하는 것입니다.\n",
    "\n",
    "- korea => korea\n",
    "- Korea => korea\n",
    "- KOREA => korea\n",
    "\n",
    "이렇게 세 단어를 하나의 통일된 단어로 바꾸게 함으로써 컴퓨터는 같은 단어임을 알 수 있습니다.\n",
    "\n",
    "Python에서는 string 변수에 `lower` 함수를 사용함으로써 소문자로 바꿀 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'korea is a region in east asia.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_text = 'Korea is a region in East Asia.'\n",
    "lower_text = original_text.lower()\n",
    "lower_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "질문) 이런 '소문자 통일' 방법이 성능을 떨어트리는 상황은 어떤 상황일까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 노이즈 제거\n",
    "다음으로 살펴볼 전처리 기술은 노이즈 제거입니다.\n",
    "\n",
    "노이즈는 파트 2에서 설명한대로 텍스트 내 불필요한 내용을 제거하는 것을 뜻합니다.\n",
    "\n",
    "노이즈로는 html 태그가 있을 수 있고 특수문자나 불필요한 정보(예를 들어 페이지 주소)등이 있을 수 있습니다.\n",
    "\n",
    "이런 데이터가 자연어처리 작업을 할 때 같이 입력으로 들어온다면 그 성능이 떨어질 것입니다.\n",
    "\n",
    "\n",
    "이러한 노이즈들은 대체로 일정 패턴을 가지고 있습니다. 파트 2에서는 html tag가 본문의 앞 뒤로 붙어있는 것을 알 수 있습니다.\n",
    "\n",
    "텍스트 내 일정한 규칙에 따라 우리가 필요로 하는 부분을 뽑아내고 바꾸는 방법으로 알려진 것이 정규표현식(Regular expression)이 있습니다.\n",
    "\n",
    "파트 2에서 소개한 예제를 정규표현식을 이용하여 노이즈 제거를 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(로마=연합뉴스) 전성훈 특파원 = 신종 코로나바이러스 감염증(코로나19) 재확산 위기에 직면한 이탈리아 정부가 25일(현지시간) 고강도 제한 조처를 추가로 내놓을 것으로 전망된다. \n",
      "일간 라 레푸블리카 등 현지 언론에 따르면 주세페 콘테 총리는 24일 긴급 당정 회의를 열어 코로나19 확산세를 꺾기 위한 추가 제한 조처를 논의했다. \n",
      "논의된 내용의 핵심은 음식점과 주점, 아이스크림 가게, 빵집 등의 주중 영업시간을 오후 6시 또는 오후 8시까지로 제한하고 일요일과 공휴일에는 영업을 전면 금지한다는 것이다. \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "extract_html_body_pattern = re.compile(r'<[^>]+?>(.+?)</[^>]+?>')\n",
    "original_text = \"\"\"\n",
    "<p dmcf-pid=\"AApwHtOKeU\" dmcf-ptype=\"general\">(로마=연합뉴스) 전성훈 특파원 = 신종 코로나바이러스 감염증(코로나19) 재확산 위기에 직면한 이탈리아 정부가 25일(현지시간) 고강도 제한 조처를 추가로 내놓을 것으로 전망된다. </p>\n",
    "<p dmcf-pid=\"AvOfaLP6Id\" dmcf-ptype=\"general\">일간 라 레푸블리카 등 현지 언론에 따르면 주세페 콘테 총리는 24일 긴급 당정 회의를 열어 코로나19 확산세를 꺾기 위한 추가 제한 조처를 논의했다. </p>\n",
    "<p dmcf-pid=\"ADmesRBWhi\" dmcf-ptype=\"general\">논의된 내용의 핵심은 음식점과 주점, 아이스크림 가게, 빵집 등의 주중 영업시간을 오후 6시 또는 오후 8시까지로 제한하고 일요일과 공휴일에는 영업을 전면 금지한다는 것이다. </p>\n",
    "\"\"\"\n",
    "body_list = extract_html_body_pattern.findall(original_text)\n",
    "html_body = \"\\n\".join(body_list)\n",
    "print(html_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토큰화(Tokenization)\n",
    "토큰이란 텍스트 데이터에서 하나의 작은 단위를 뜻합니다. 이 이상 쪼갤 수 없는 단위라고 보시면 되겠습니다.\n",
    "\n",
    "우리는 이러한 토큰이 모여서 하나의 텍스트가 된다고 볼 수 있습니다.\n",
    "\n",
    "토큰은 어플리케이션에 따라 혹은 사람이 정하기에 따라 달라집니다. 문장이 될 수 있고 단어가 될 수 있고 문자가 될 수 있습니다.\n",
    "\n",
    "예를 들어 아래와 같은 텍스트가 있을 때 우리는 이와 같이 토큰을 만들 수 있습니다.\n",
    "\n",
    "\n",
    "| Korea is a region in East Asia. Since 1945 it has been divided into two states. |\n",
    "| - |\n",
    "\n",
    "\n",
    "- 문장\n",
    "\n",
    "`Korea is a region in East Asia`, `Since 1945 it has been divided into two states`\n",
    "\n",
    "- 단어\n",
    "\n",
    "`Korea`, `is`, `a`, `region`, `in`, `East`, `Asia`, `Since`, `1945`, `it`, `has`, `been`, `divided`, `into`, `two`, `states`\n",
    "  \n",
    "- 문자 (알파벳)\n",
    "\n",
    "`K`, `o`, `r`, `e`, `a`, `i`, `s`, `a`, `r`, `e`, `g`, `i`, `o`, `n`, `i`, `n`, `E`, `a`, `s`, `t`, `A`, `s`, `i`, `a`, `S`, `i`, `n`, `c`, `e`, `1`, `9`, `4`, `5`, `i`, `t`, `h`, `a`, `s`, `b`, `e`, `e`, `n`, `d`, `i`, `v`, `i`, `d`, `e`, `d`, `i`, `n`, `t`, `o`, `t`, `w`, `o`, `s`, `t`, `a`, `t`, `e`, `s`\n",
    "\n",
    "어떤 단위로 토큰을 할 것이냐에 따라 텍스트의 형태가 달라진다고 볼 수 있습니다.\n",
    "\n",
    "아래 코드는 문장과 단어 단위로 토큰을 만드는 코드입니다. 토큰화를 위해 Natural Language Toolkit(NLTK) 라이브러리를 사용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\302-26\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # 토큰화를 위해 필요한 파일 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "original_text = 'Korea is a region in East Asia. Since 1945 it has been divided into two states.'\n",
    "sentence_token = sent_tokenize(original_text)\n",
    "\n",
    "print(sentence_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_token = word_tokenize(original_text)\n",
    "\n",
    "print(word_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 실행 결과 두 문장의 텍스트가 문장 단위와 단어 단위로 쪼개지는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stop-words 제거\n",
    "다음으로 살펴볼 전처리 기술은 stop-words 제거입니다.\n",
    "\n",
    "stop-word란 흔히들 자주 쓰이는 단어를 뜻합니다. 영어의 경우 a, the, is, are와 같은 단어들입니다. 이러한 단어들은 대체로 담고있는 정보가 거의 없기에 생략을 하여도 텍스트를 이해함에 있어 크게 문제가 되지 않습니다.\n",
    "\n",
    "이러한 stop-word는 대체로 언어들마다 정리되어 있습니다. \n",
    "\n",
    "아래 코드에서 Natural Language Toolkit(NLTK) 라이브러리 내 정의된 영어의 stop-word를 사용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "original_text = 'Korea is a region in East Asia.'\n",
    "word_token = word_tokenize(original_text)\n",
    "filtered_text = [word for word in word_token if not word in stop_words]\n",
    "\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 결과 본래 문장에서 Korea, region, East, Asia의 단어들이 남고 나머지는 stop-word이기에 제거된 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 표준화(Normalization)\n",
    "표준화는 텍스트 전처리 기술의 핵심이라고 볼 수 있습니다. 이것은 단어들을 좀 더 알아보기 쉽게 바꾸는 작업을 뜻합니다.\n",
    "\n",
    "예를 들어 단어 원형을 찾는 어간 추출(Stemming 혹은 Lemmatization)이 있습니다. 어간 추출의 예시는 아래와 같습니다.\n",
    "\n",
    "- eat\n",
    "- eating\n",
    "- ate\n",
    "- eaten\n",
    "- eats\n",
    "\n",
    "위 단어들은 전부 먹다라는 뜻의 eat 동사의 다양한 형태입니다. 각각의 단어들은 먹다라는 뜻 외에 과거형인지 현재진행형인지 등의 정보를 담고 있지만, 먹다라는 정보가 중요하다면 이들을 하나의 단어로 바꾸는 것이 중요합니다. \n",
    "이는 앞서 살펴보면 소문자로 통일의 전처리와 같은 목적을 가집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드에서 Natural Language Toolkit(NLTK) 라이브러리 내 stemmer를 이용하여 stemming을 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "original_text = 'He eats chicken and I am eating pizza'\n",
    "word_token = word_tokenize(original_text)\n",
    "stemming_text = [stemmer.stem(word) for word in word_token]\n",
    "\n",
    "print(stemming_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 실행 결과 각각의 단어의 원형이 나옴을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표준화에는 이러한 동사 원형을 찾는 것 외에 다양합니다.\n",
    "\n",
    "- 인터넷 용어 원형(예: b4 => before)\n",
    "- 이모티콘 통일(예: :), :-), ^^ => smile_emoticon)\n",
    "- 오탈자 처리(예: reapet => repeat)\n",
    "- 특수 문자 치환(예: 텍스트 내 모든 URL들을 삭제하지 않고 '##URL##'이라는 특수 문자로 치환하여 URL이 있음을 남김)\n",
    "- 자주 등장하지 않는 단어 제거/치환(예: 가끔 나타나는 단어는 학습이 어렵기에 노이즈로 생각하여 삭제 혹은 있음을 남기기 위해 특수 문자로 치환)\n",
    "\n",
    "이처럼 다양한 전처리 기술이 있습니다. 텍스트 데이터 및 문제가 어떤 특징을 가지는지에 따라 적합한 전처리를 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:rgb(120, 120, 120)\">본 학습 자료를 포함한 사이트 내 모든 자료의 저작권은 엘리스에 있으며 외부로의 무단 복제, 배포 및 전송을 불허합니다.\n",
    "\n",
    "Copyright @ elice all rights reserved</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
